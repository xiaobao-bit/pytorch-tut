{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9899868-6f1b-4ab2-83f4-17c3ce03fa77",
   "metadata": {},
   "source": [
    "## Linear Regression via Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf0b88-35d6-40af-acc2-1667fd4e8241",
   "metadata": {},
   "source": [
    "In previous part, the NN model is built without using any torch frameworks. Therefore in this part, the torch tool will be used to build the NN.  \n",
    "\n",
    "The main advantage is that by using Pytorch, we can no longer focus on calculating grad, loss, etc. and we can **focus on building our computational graph + structure of the NN**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94c2ea3b-7cd1-4e20-b9f6-fb71c85a7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0387ec-42ec-4ea4-8ba7-ee3381d55a0d",
   "metadata": {},
   "source": [
    "### Step 1: Define the model using torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04e2620f-fa7d-4d95-aa85-7418df005463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        \n",
    "        # The model has only 1 input and 1 output\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_predict = self.linear(x)\n",
    "        return y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ddb2b-4960-408e-baa5-2f040078b249",
   "metadata": {},
   "source": [
    "Then we assume some input and output (same as before), however in order to put them into the NN, their type has to be tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b884962c-710c-4cf6-b968-1765cb5b0076",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.tensor([[2.0], [4.0], [6.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6892ab-0dfb-4a47-bb51-265dabaab9b5",
   "metadata": {},
   "source": [
    "### Step 2: Instantiate the model and define loss function + optimizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1919773-48c4-4878-b2ad-bce05ea5ab57",
   "metadata": {},
   "source": [
    "Instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "795eb338-c9ab-41a0-bb6d-b2c5f81703cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5916f9ac-18e3-4f7f-ab8f-f4db8a86cec3",
   "metadata": {},
   "source": [
    "Define loss function (MSE here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbdf4f9b-81d4-40e5-b094-6bdcb43d501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(reduction = 'sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5d1fa-d35a-4aef-a2b8-15441bd83be3",
   "metadata": {},
   "source": [
    "Define optimizer (dont't dive into how does it work, just do it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99f11e83-8884-48e8-8837-eab30000287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac464d-4d90-4de4-bbbb-bf6bb1028d60",
   "metadata": {},
   "source": [
    "### Step 3: Training the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068c1e84-f2ae-4059-a806-8f792d1cad21",
   "metadata": {},
   "source": [
    "Keep in mind that the training always consists of **forward + backward + update**, where  \n",
    "**forward:** for predict the output and calcuate the loss  \n",
    "**backward:** for get the grad (auto-grad here)  \n",
    "**update:** for updating the *weight* and *bias*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "447c154b-362a-41a0-8de9-23fb9a894e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \t Loss:  60.22477722167969\n",
      "Epoch:  1 \t Loss:  26.867839813232422\n",
      "Epoch:  2 \t Loss:  12.017452239990234\n",
      "Epoch:  3 \t Loss:  5.405668258666992\n",
      "Epoch:  4 \t Loss:  2.4614882469177246\n",
      "Epoch:  5 \t Loss:  1.1500314474105835\n",
      "Epoch:  6 \t Loss:  0.5654281377792358\n",
      "Epoch:  7 \t Loss:  0.3044118881225586\n",
      "Epoch:  8 \t Loss:  0.18745721876621246\n",
      "Epoch:  9 \t Loss:  0.13464577496051788\n",
      "Epoch:  10 \t Loss:  0.11040014028549194\n",
      "Epoch:  11 \t Loss:  0.09888136386871338\n",
      "Epoch:  12 \t Loss:  0.0930386483669281\n",
      "Epoch:  13 \t Loss:  0.08973316848278046\n",
      "Epoch:  14 \t Loss:  0.08756745606660843\n",
      "Epoch:  15 \t Loss:  0.08591876924037933\n",
      "Epoch:  16 \t Loss:  0.08451025933027267\n",
      "Epoch:  17 \t Loss:  0.08321855962276459\n",
      "Epoch:  18 \t Loss:  0.08198798447847366\n",
      "Epoch:  19 \t Loss:  0.08079448342323303\n",
      "Epoch:  20 \t Loss:  0.0796264261007309\n",
      "Epoch:  21 \t Loss:  0.07847915589809418\n",
      "Epoch:  22 \t Loss:  0.0773499608039856\n",
      "Epoch:  23 \t Loss:  0.07623769342899323\n",
      "Epoch:  24 \t Loss:  0.07514171302318573\n",
      "Epoch:  25 \t Loss:  0.0740617960691452\n",
      "Epoch:  26 \t Loss:  0.07299727201461792\n",
      "Epoch:  27 \t Loss:  0.07194822281599045\n",
      "Epoch:  28 \t Loss:  0.07091414928436279\n",
      "Epoch:  29 \t Loss:  0.06989498436450958\n",
      "Epoch:  30 \t Loss:  0.06889059394598007\n",
      "Epoch:  31 \t Loss:  0.06790033727884293\n",
      "Epoch:  32 \t Loss:  0.06692458689212799\n",
      "Epoch:  33 \t Loss:  0.06596280634403229\n",
      "Epoch:  34 \t Loss:  0.06501469016075134\n",
      "Epoch:  35 \t Loss:  0.06408046185970306\n",
      "Epoch:  36 \t Loss:  0.06315946578979492\n",
      "Epoch:  37 \t Loss:  0.06225176900625229\n",
      "Epoch:  38 \t Loss:  0.06135720759630203\n",
      "Epoch:  39 \t Loss:  0.060475386679172516\n",
      "Epoch:  40 \t Loss:  0.05960610508918762\n",
      "Epoch:  41 \t Loss:  0.05874944478273392\n",
      "Epoch:  42 \t Loss:  0.057905230671167374\n",
      "Epoch:  43 \t Loss:  0.05707294121384621\n",
      "Epoch:  44 \t Loss:  0.056252773851156235\n",
      "Epoch:  45 \t Loss:  0.055444471538066864\n",
      "Epoch:  46 \t Loss:  0.05464752018451691\n",
      "Epoch:  47 \t Loss:  0.053862154483795166\n",
      "Epoch:  48 \t Loss:  0.05308809131383896\n",
      "Epoch:  49 \t Loss:  0.05232508108019829\n",
      "Epoch:  50 \t Loss:  0.05157316103577614\n",
      "Epoch:  51 \t Loss:  0.05083194747567177\n",
      "Epoch:  52 \t Loss:  0.05010133981704712\n",
      "Epoch:  53 \t Loss:  0.04938125237822533\n",
      "Epoch:  54 \t Loss:  0.04867170378565788\n",
      "Epoch:  55 \t Loss:  0.04797228425741196\n",
      "Epoch:  56 \t Loss:  0.04728272557258606\n",
      "Epoch:  57 \t Loss:  0.046603161841630936\n",
      "Epoch:  58 \t Loss:  0.04593338817358017\n",
      "Epoch:  59 \t Loss:  0.0452733151614666\n",
      "Epoch:  60 \t Loss:  0.044622719287872314\n",
      "Epoch:  61 \t Loss:  0.043981362134218216\n",
      "Epoch:  62 \t Loss:  0.04334932565689087\n",
      "Epoch:  63 \t Loss:  0.04272637888789177\n",
      "Epoch:  64 \t Loss:  0.042112238705158234\n",
      "Epoch:  65 \t Loss:  0.04150693863630295\n",
      "Epoch:  66 \t Loss:  0.04091053456068039\n",
      "Epoch:  67 \t Loss:  0.04032262787222862\n",
      "Epoch:  68 \t Loss:  0.03974301367998123\n",
      "Epoch:  69 \t Loss:  0.03917181119322777\n",
      "Epoch:  70 \t Loss:  0.03860887140035629\n",
      "Epoch:  71 \t Loss:  0.038054000586271286\n",
      "Epoch:  72 \t Loss:  0.037507154047489166\n",
      "Epoch:  73 \t Loss:  0.036968111991882324\n",
      "Epoch:  74 \t Loss:  0.0364367738366127\n",
      "Epoch:  75 \t Loss:  0.035913173109292984\n",
      "Epoch:  76 \t Loss:  0.03539704158902168\n",
      "Epoch:  77 \t Loss:  0.034888386726379395\n",
      "Epoch:  78 \t Loss:  0.03438693284988403\n",
      "Epoch:  79 \t Loss:  0.033892713487148285\n",
      "Epoch:  80 \t Loss:  0.03340565413236618\n",
      "Epoch:  81 \t Loss:  0.032925598323345184\n",
      "Epoch:  82 \t Loss:  0.03245236352086067\n",
      "Epoch:  83 \t Loss:  0.03198598325252533\n",
      "Epoch:  84 \t Loss:  0.03152628242969513\n",
      "Epoch:  85 \t Loss:  0.031073138117790222\n",
      "Epoch:  86 \t Loss:  0.030626630410552025\n",
      "Epoch:  87 \t Loss:  0.03018643520772457\n",
      "Epoch:  88 \t Loss:  0.029752636328339577\n",
      "Epoch:  89 \t Loss:  0.029325034469366074\n",
      "Epoch:  90 \t Loss:  0.028903581202030182\n",
      "Epoch:  91 \t Loss:  0.028488242998719215\n",
      "Epoch:  92 \t Loss:  0.02807880938053131\n",
      "Epoch:  93 \t Loss:  0.02767517790198326\n",
      "Epoch:  94 \t Loss:  0.027277596294879913\n",
      "Epoch:  95 \t Loss:  0.026885420083999634\n",
      "Epoch:  96 \t Loss:  0.026499120518565178\n",
      "Epoch:  97 \t Loss:  0.0261182002723217\n",
      "Epoch:  98 \t Loss:  0.025742901489138603\n",
      "Epoch:  99 \t Loss:  0.025372933596372604\n",
      "Epoch:  100 \t Loss:  0.02500827983021736\n",
      "Epoch:  101 \t Loss:  0.024648848921060562\n",
      "Epoch:  102 \t Loss:  0.0242946557700634\n",
      "Epoch:  103 \t Loss:  0.02394544705748558\n",
      "Epoch:  104 \t Loss:  0.023601368069648743\n",
      "Epoch:  105 \t Loss:  0.023262161761522293\n",
      "Epoch:  106 \t Loss:  0.022927843034267426\n",
      "Epoch:  107 \t Loss:  0.022598380222916603\n",
      "Epoch:  108 \t Loss:  0.022273540496826172\n",
      "Epoch:  109 \t Loss:  0.02195345051586628\n",
      "Epoch:  110 \t Loss:  0.021637992933392525\n",
      "Epoch:  111 \t Loss:  0.021326949819922447\n",
      "Epoch:  112 \t Loss:  0.02102043479681015\n",
      "Epoch:  113 \t Loss:  0.020718416199088097\n",
      "Epoch:  114 \t Loss:  0.020420599728822708\n",
      "Epoch:  115 \t Loss:  0.020127175375819206\n",
      "Epoch:  116 \t Loss:  0.01983788050711155\n",
      "Epoch:  117 \t Loss:  0.019552763551473618\n",
      "Epoch:  118 \t Loss:  0.01927179843187332\n",
      "Epoch:  119 \t Loss:  0.018994847312569618\n",
      "Epoch:  120 \t Loss:  0.018721889704465866\n",
      "Epoch:  121 \t Loss:  0.018452715128660202\n",
      "Epoch:  122 \t Loss:  0.018187591806054115\n",
      "Epoch:  123 \t Loss:  0.01792619377374649\n",
      "Epoch:  124 \t Loss:  0.017668601125478745\n",
      "Epoch:  125 \t Loss:  0.017414648085832596\n",
      "Epoch:  126 \t Loss:  0.017164355143904686\n",
      "Epoch:  127 \t Loss:  0.016917679458856583\n",
      "Epoch:  128 \t Loss:  0.016674598678946495\n",
      "Epoch:  129 \t Loss:  0.016434932127594948\n",
      "Epoch:  130 \t Loss:  0.016198724508285522\n",
      "Epoch:  131 \t Loss:  0.015965916216373444\n",
      "Epoch:  132 \t Loss:  0.015736494213342667\n",
      "Epoch:  133 \t Loss:  0.015510338358581066\n",
      "Epoch:  134 \t Loss:  0.01528739370405674\n",
      "Epoch:  135 \t Loss:  0.015067669562995434\n",
      "Epoch:  136 \t Loss:  0.014851151965558529\n",
      "Epoch:  137 \t Loss:  0.01463775523006916\n",
      "Epoch:  138 \t Loss:  0.014427334070205688\n",
      "Epoch:  139 \t Loss:  0.014220017939805984\n",
      "Epoch:  140 \t Loss:  0.014015620574355125\n",
      "Epoch:  141 \t Loss:  0.013814253732562065\n",
      "Epoch:  142 \t Loss:  0.013615693897008896\n",
      "Epoch:  143 \t Loss:  0.013419992290437222\n",
      "Epoch:  144 \t Loss:  0.013227159157395363\n",
      "Epoch:  145 \t Loss:  0.013037102296948433\n",
      "Epoch:  146 \t Loss:  0.012849711813032627\n",
      "Epoch:  147 \t Loss:  0.01266498677432537\n",
      "Epoch:  148 \t Loss:  0.012482964433729649\n",
      "Epoch:  149 \t Loss:  0.012303652241826057\n",
      "Epoch:  150 \t Loss:  0.012126744724810123\n",
      "Epoch:  151 \t Loss:  0.011952497996389866\n",
      "Epoch:  152 \t Loss:  0.01178072951734066\n",
      "Epoch:  153 \t Loss:  0.01161141972988844\n",
      "Epoch:  154 \t Loss:  0.011444592848420143\n",
      "Epoch:  155 \t Loss:  0.011280041188001633\n",
      "Epoch:  156 \t Loss:  0.011117999441921711\n",
      "Epoch:  157 \t Loss:  0.010958151891827583\n",
      "Epoch:  158 \t Loss:  0.010800652205944061\n",
      "Epoch:  159 \t Loss:  0.010645468719303608\n",
      "Epoch:  160 \t Loss:  0.010492450557649136\n",
      "Epoch:  161 \t Loss:  0.010341716930270195\n",
      "Epoch:  162 \t Loss:  0.01019303034991026\n",
      "Epoch:  163 \t Loss:  0.010046563111245632\n",
      "Epoch:  164 \t Loss:  0.009902231395244598\n",
      "Epoch:  165 \t Loss:  0.009759903885424137\n",
      "Epoch:  166 \t Loss:  0.009619609452784061\n",
      "Epoch:  167 \t Loss:  0.009481395594775677\n",
      "Epoch:  168 \t Loss:  0.00934507604688406\n",
      "Epoch:  169 \t Loss:  0.009210757911205292\n",
      "Epoch:  170 \t Loss:  0.009078421629965305\n",
      "Epoch:  171 \t Loss:  0.008947949856519699\n",
      "Epoch:  172 \t Loss:  0.008819318376481533\n",
      "Epoch:  173 \t Loss:  0.00869259424507618\n",
      "Epoch:  174 \t Loss:  0.008567674085497856\n",
      "Epoch:  175 \t Loss:  0.008444562554359436\n",
      "Epoch:  176 \t Loss:  0.008323188871145248\n",
      "Epoch:  177 \t Loss:  0.00820354837924242\n",
      "Epoch:  178 \t Loss:  0.008085682988166809\n",
      "Epoch:  179 \t Loss:  0.007969480007886887\n",
      "Epoch:  180 \t Loss:  0.007854928262531757\n",
      "Epoch:  181 \t Loss:  0.0077420384623110294\n",
      "Epoch:  182 \t Loss:  0.007630804553627968\n",
      "Epoch:  183 \t Loss:  0.007521091029047966\n",
      "Epoch:  184 \t Loss:  0.007413043174892664\n",
      "Epoch:  185 \t Loss:  0.007306468673050404\n",
      "Epoch:  186 \t Loss:  0.0072014546021819115\n",
      "Epoch:  187 \t Loss:  0.007097989786416292\n",
      "Epoch:  188 \t Loss:  0.0069959731772542\n",
      "Epoch:  189 \t Loss:  0.006895415019243956\n",
      "Epoch:  190 \t Loss:  0.006796298548579216\n",
      "Epoch:  191 \t Loss:  0.0066986423917114735\n",
      "Epoch:  192 \t Loss:  0.006602345034480095\n",
      "Epoch:  193 \t Loss:  0.00650746189057827\n",
      "Epoch:  194 \t Loss:  0.006413964554667473\n",
      "Epoch:  195 \t Loss:  0.006321818567812443\n",
      "Epoch:  196 \t Loss:  0.006230945233255625\n",
      "Epoch:  197 \t Loss:  0.00614140834659338\n",
      "Epoch:  198 \t Loss:  0.006053094752132893\n",
      "Epoch:  199 \t Loss:  0.005966156255453825\n",
      "Epoch:  200 \t Loss:  0.005880418233573437\n",
      "Epoch:  201 \t Loss:  0.005795853212475777\n",
      "Epoch:  202 \t Loss:  0.005712565500289202\n",
      "Epoch:  203 \t Loss:  0.005630473140627146\n",
      "Epoch:  204 \t Loss:  0.005549550522118807\n",
      "Epoch:  205 \t Loss:  0.005469850730150938\n",
      "Epoch:  206 \t Loss:  0.005391222424805164\n",
      "Epoch:  207 \t Loss:  0.005313714500516653\n",
      "Epoch:  208 \t Loss:  0.005237344186753035\n",
      "Epoch:  209 \t Loss:  0.005162094719707966\n",
      "Epoch:  210 \t Loss:  0.0050878748297691345\n",
      "Epoch:  211 \t Loss:  0.005014777649194002\n",
      "Epoch:  212 \t Loss:  0.004942703992128372\n",
      "Epoch:  213 \t Loss:  0.0048716627061367035\n",
      "Epoch:  214 \t Loss:  0.004801684059202671\n",
      "Epoch:  215 \t Loss:  0.004732675850391388\n",
      "Epoch:  216 \t Loss:  0.004664626903831959\n",
      "Epoch:  217 \t Loss:  0.004597595892846584\n",
      "Epoch:  218 \t Loss:  0.004531519487500191\n",
      "Epoch:  219 \t Loss:  0.0044663758017122746\n",
      "Epoch:  220 \t Loss:  0.0044022053480148315\n",
      "Epoch:  221 \t Loss:  0.00433895131573081\n",
      "Epoch:  222 \t Loss:  0.004276562482118607\n",
      "Epoch:  223 \t Loss:  0.004215126391500235\n",
      "Epoch:  224 \t Loss:  0.004154517315328121\n",
      "Epoch:  225 \t Loss:  0.00409483490511775\n",
      "Epoch:  226 \t Loss:  0.004035957157611847\n",
      "Epoch:  227 \t Loss:  0.003978008404374123\n",
      "Epoch:  228 \t Loss:  0.003920810297131538\n",
      "Epoch:  229 \t Loss:  0.003864445025101304\n",
      "Epoch:  230 \t Loss:  0.003808925859630108\n",
      "Epoch:  231 \t Loss:  0.0037541540805250406\n",
      "Epoch:  232 \t Loss:  0.0037002526223659515\n",
      "Epoch:  233 \t Loss:  0.0036470501217991114\n",
      "Epoch:  234 \t Loss:  0.0035946303978562355\n",
      "Epoch:  235 \t Loss:  0.0035429811105132103\n",
      "Epoch:  236 \t Loss:  0.003492059651762247\n",
      "Epoch:  237 \t Loss:  0.003441861132159829\n",
      "Epoch:  238 \t Loss:  0.003392415586858988\n",
      "Epoch:  239 \t Loss:  0.003343667834997177\n",
      "Epoch:  240 \t Loss:  0.003295579692348838\n",
      "Epoch:  241 \t Loss:  0.003248233813792467\n",
      "Epoch:  242 \t Loss:  0.003201550105586648\n",
      "Epoch:  243 \t Loss:  0.0031555439345538616\n",
      "Epoch:  244 \t Loss:  0.00311019504442811\n",
      "Epoch:  245 \t Loss:  0.0030654992442578077\n",
      "Epoch:  246 \t Loss:  0.0030214465223252773\n",
      "Epoch:  247 \t Loss:  0.0029780110344290733\n",
      "Epoch:  248 \t Loss:  0.0029352018609642982\n",
      "Epoch:  249 \t Loss:  0.0028930308762937784\n",
      "Epoch:  250 \t Loss:  0.002851441502571106\n",
      "Epoch:  251 \t Loss:  0.0028104770462960005\n",
      "Epoch:  252 \t Loss:  0.0027700813952833414\n",
      "Epoch:  253 \t Loss:  0.0027302512899041176\n",
      "Epoch:  254 \t Loss:  0.002691032364964485\n",
      "Epoch:  255 \t Loss:  0.0026523577980697155\n",
      "Epoch:  256 \t Loss:  0.0026142278220504522\n",
      "Epoch:  257 \t Loss:  0.002576662926003337\n",
      "Epoch:  258 \t Loss:  0.002539660083130002\n",
      "Epoch:  259 \t Loss:  0.0025031575933098793\n",
      "Epoch:  260 \t Loss:  0.002467153361067176\n",
      "Epoch:  261 \t Loss:  0.00243169954046607\n",
      "Epoch:  262 \t Loss:  0.0023967698216438293\n",
      "Epoch:  263 \t Loss:  0.0023623043671250343\n",
      "Epoch:  264 \t Loss:  0.00232836976647377\n",
      "Epoch:  265 \t Loss:  0.0022949003614485264\n",
      "Epoch:  266 \t Loss:  0.0022619222290813923\n",
      "Epoch:  267 \t Loss:  0.0022294295486062765\n",
      "Epoch:  268 \t Loss:  0.0021973822731524706\n",
      "Epoch:  269 \t Loss:  0.002165804849937558\n",
      "Epoch:  270 \t Loss:  0.0021346763242036104\n",
      "Epoch:  271 \t Loss:  0.0021039864514023066\n",
      "Epoch:  272 \t Loss:  0.002073749201372266\n",
      "Epoch:  273 \t Loss:  0.0020439540967345238\n",
      "Epoch:  274 \t Loss:  0.0020145783200860023\n",
      "Epoch:  275 \t Loss:  0.0019856069702655077\n",
      "Epoch:  276 \t Loss:  0.001957089640200138\n",
      "Epoch:  277 \t Loss:  0.001928960089571774\n",
      "Epoch:  278 \t Loss:  0.0019012342672795057\n",
      "Epoch:  279 \t Loss:  0.0018739054212346673\n",
      "Epoch:  280 \t Loss:  0.0018469745991751552\n",
      "Epoch:  281 \t Loss:  0.0018204369116574526\n",
      "Epoch:  282 \t Loss:  0.0017942637205123901\n",
      "Epoch:  283 \t Loss:  0.0017684830818325281\n",
      "Epoch:  284 \t Loss:  0.0017430635634809732\n",
      "Epoch:  285 \t Loss:  0.001718028332106769\n",
      "Epoch:  286 \t Loss:  0.0016933225560933352\n",
      "Epoch:  287 \t Loss:  0.0016690068878233433\n",
      "Epoch:  288 \t Loss:  0.0016450182301923633\n",
      "Epoch:  289 \t Loss:  0.001621369388885796\n",
      "Epoch:  290 \t Loss:  0.001598077593371272\n",
      "Epoch:  291 \t Loss:  0.0015751090832054615\n",
      "Epoch:  292 \t Loss:  0.0015524807386100292\n",
      "Epoch:  293 \t Loss:  0.0015301571693271399\n",
      "Epoch:  294 \t Loss:  0.0015081558376550674\n",
      "Epoch:  295 \t Loss:  0.0014864817494526505\n",
      "Epoch:  296 \t Loss:  0.0014651380479335785\n",
      "Epoch:  297 \t Loss:  0.0014440594241023064\n",
      "Epoch:  298 \t Loss:  0.0014233110705390573\n",
      "Epoch:  299 \t Loss:  0.0014028479345142841\n"
     ]
    }
   ],
   "source": [
    "# training cycle forward, backward, update\n",
    "for epoch in range(300):\n",
    "    y_pred = model(x_data)  # forward: predict\n",
    "    loss = criterion(y_pred, y_data)  # forward: loss\n",
    "    print('Epoch: ', epoch, '\\t Loss: ', loss.item())\n",
    "\n",
    "    optimizer.zero_grad()  # the grad computer by .backward() will be accumulated. so before backward, remember set the grad to zero\n",
    "    loss.backward()  # backward: autograd，自动计算梯度\n",
    "    optimizer.step()  # update 参数，即更新w和b的值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1350eb0f-990f-4183-9413-2ac1d7d40969",
   "metadata": {},
   "source": [
    "### Step 4: Show the result and validate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "838130ad-24ba-42f1-9bf5-2336abe09a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  1.9750655889511108\n",
      "b =  0.056681737303733826\n",
      "y_pred =  tensor([[7.9569]])\n"
     ]
    }
   ],
   "source": [
    "print('w = ', model.linear.weight.item())\n",
    "print('b = ', model.linear.bias.item())\n",
    "\n",
    "x_test = torch.tensor([[4.0]])\n",
    "y_test = model(x_test)\n",
    "print('y_pred = ', y_test.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
